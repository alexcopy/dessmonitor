# ML Quick Start Guide

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç –∑–∞ 5 —à–∞–≥–æ–≤

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è ML
pip install pandas numpy scikit-learn matplotlib

# –ò–ª–∏ –≤—Å–µ —Å—Ä–∞–∑—É –∏–∑ —Ñ–∞–π–ª–∞
pip install -r requirements-ml.txt
```

### 2. –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã

```bash
# –°–æ–∑–¥–∞–π—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p app/ml
mkdir -p ml_data/models
mkdir -p ml_data/archive

# –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –º–æ–¥—É–ª–µ–π
# - app/ml/ml_data_collector.py
# - app/ml/ml_data_analyzer.py
# - app/ml/ml_model_training_example.py
# - app/service/ml_smart_controller.py
```

### 3. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–±–æ—Ä—â–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ run.py

```python
# –í run.py –¥–æ–±–∞–≤—å—Ç–µ –∏–º–ø–æ—Ä—Ç—ã
from app.ml.ml_data_collector import MLDataCollector, ml_collection_loop

# –í —Ñ—É–Ω–∫—Ü–∏–∏ main() –¥–æ–±–∞–≤—å—Ç–µ:
ml_collector = MLDataCollector(
    csv_path=Path("ml_data/training_data.csv"),
    collect_interval=300,  # 5 –º–∏–Ω—É—Ç
)

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∫–∞–∫ –∫–æ—Ä—É—Ç–∏–Ω—É
ml_task = asyncio.create_task(
    ml_collection_loop(ml_collector, dev_mgr, stop_event)
)

# –í –±–ª–æ–∫–µ finally –¥–æ–±–∞–≤—å—Ç–µ –æ—Å—Ç–∞–Ω–æ–≤–∫—É
ml_task.cancel()
await asyncio.gather(..., ml_task, return_exceptions=True)
```

### 4. –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
python run.py

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è
ls -lh ml_data/training_data.csv

# –°–ª–µ–¥–∏—Ç–µ –∑–∞ –ª–æ–≥–∞–º–∏
tail -f logs/important.log | grep ML
```

### 5. –ü–µ—Ä–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ (–ø–æ—Å–ª–µ –Ω–µ–¥–µ–ª–∏ —Å–±–æ—Ä–∞)

```bash
# –ê–Ω–∞–ª–∏–∑ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
python app/ml/ml_data_analyzer.py stats

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
python app/ml/ml_data_analyzer.py gaps

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
python app/ml/ml_data_analyzer.py plot

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
python app/ml/ml_data_analyzer.py export

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
python app/ml/ml_model_training_example.py all

# –¢–µ—Å—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
python app/ml/ml_model_training_example.py test
```

---

## –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã

### –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –î–∞–Ω–Ω—ã–µ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è?

```bash
# –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ —Ä–∞—Å—Ç—ë—Ç?
watch -n 60 'ls -lh ml_data/training_data.csv'

# –ü–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø–∏—Å–∏
tail -5 ml_data/training_data.csv

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
wc -l ml_data/training_data.csv
```

**–û–∂–∏–¥–∞–µ–º–æ:** +12 –∑–∞–ø–∏—Å–µ–π –≤ —á–∞—Å (–∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç)

### –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö

```python
from app.ml.ml_data_analyzer import MLDataAnalyzer

analyzer = MLDataAnalyzer()
stats = analyzer.basic_statistics()

print(f"Records: {stats['total_records']}")
print(f"Days: {stats['date_range_days']}")
print(f"Missing values: {stats['missing_values']}")
```

**–û–∂–∏–¥–∞–µ–º–æ:** 
- –ü–æ—Å–ª–µ 1 –¥–Ω—è: ~288 –∑–∞–ø–∏—Å–µ–π
- –ü–æ—Å–ª–µ 1 –Ω–µ–¥–µ–ª–∏: ~2016 –∑–∞–ø–∏—Å–µ–π
- –ü—Ä–æ–ø—É—Å–∫–æ–≤ < 5%

### –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã?

```bash
ls -lh ml_data/models/

# –î–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ñ–∞–π–ª—ã:
# - pv_predictor.pkl
# - battery_optimizer.pkl
# - pump_controller.pkl (–µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞—Å–æ—Å)
```

---

## –¢–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### –ü—Ä–æ–±–ª–µ–º–∞: FileNotFoundError –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏

```
FileNotFoundError: Training data not found: ml_data/train_data.csv
```

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# –°–Ω–∞—á–∞–ª–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ
python app/ml/ml_data_analyzer.py export

# –ó–∞—Ç–µ–º –æ–±—É—á–∞–π—Ç–µ
python app/ml/ml_model_training_example.py all
```

### –ü—Ä–æ–±–ª–µ–º–∞: –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö

```
ValueError: train_test_split: n_samples=50 should be >= n_splits=5
```

**–†–µ—à–µ–Ω–∏–µ:**
- –ü–æ–¥–æ–∂–¥–∏—Ç–µ, –ø–æ–∫–∞ –Ω–∞–±–µ—Ä—ë—Ç—Å—è –º–∏–Ω–∏–º—É–º **1 –Ω–µ–¥–µ–ª—è** –¥–∞–Ω–Ω—ã—Ö (~2000 –∑–∞–ø–∏—Å–µ–π)
- –ò–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ test_split –¥–æ 0.1 –≤ ml_data_analyzer.py

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∏–∑–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å

**–ü—Ä–∏–º–µ—Ä:** Test R¬≤ = 0.3 (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å > 0.7)

**–†–µ—à–µ–Ω–∏–µ:**
1. –°–æ–±–µ—Ä–∏—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö (–º–∏–Ω–∏–º—É–º 1 –º–µ—Å—è—Ü)
2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–æ–ø—É—Å–∫–∏: `python app/ml/ml_data_analyzer.py gaps`
3. –£–±–µ–¥–∏—Ç–µ—Å—å –≤ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ (—Å–æ–ª–Ω—Ü–µ, –æ–±–ª–∞—á–Ω–æ—Å—Ç—å, –Ω–æ—á—å, —Ä–∞–∑–Ω—ã–µ –Ω–∞–≥—Ä—É–∑–∫–∏)

### –ü—Ä–æ–±–ª–µ–º–∞: ML-–º–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ

```
RuntimeError: ML models not available
```

**–†–µ—à–µ–Ω–∏–µ:**
```python
# –í run.py –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–∂–∏–º HYBRID (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
ml_smart_ctrl = MLSmartController(
    mode="HYBRID",  # –Ω–µ ML_ONLY!
    ...
)

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
ls ml_data/models/*.pkl
```

---

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ML-—Å–∏—Å—Ç–µ–º—ã

### Grafana Dashboard (–ø—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞ Loki)

```logql
# –í—Å–µ —Å–æ–±—ã—Ç–∏—è ML-–∫–æ–ª–ª–µ–∫—Ç–æ—Ä–∞
{job="dessmonitor"} |= "MLDataCollector"

# –ú–µ—Ç—Ä–∏–∫–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
{job="dessmonitor"} | logfmt | type="device_metrics"

# –ú–µ—Ç—Ä–∏–∫–∏ –∏–Ω–≤–µ—Ä—Ç–æ—Ä–∞
{job="dessmonitor"} | logfmt | type="inverter"

# –†–µ—à–µ–Ω–∏—è ML-–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
{job="dessmonitor"} |= "MLController"
```

### –ê–ª–µ—Ä—Ç—ã –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏

```python
# –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å:
# app/ml/anomaly_detector.py

from sklearn.ensemble import IsolationForest

# –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π PV/Battery/Load
detector = IsolationForest(contamination=0.05)
detector.fit(normal_data)

is_anomaly = detector.predict(current_data)
if is_anomaly == -1:
    logger.warning("‚ö†Ô∏è Anomaly detected!")
```

---

## –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

```python
# –í ml_data_analyzer.py, –º–µ—Ç–æ–¥ create_features()

# –ü—Ä–∏–º–µ—Ä: –¥–æ–±–∞–≤–∏—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä "–ø–∏–∫ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è"
df['is_peak_hours'] = df['hour'].isin([18, 19, 20]).astype(int)

# –ü—Ä–∏–º–µ—Ä: –¥–æ–±–∞–≤–∏—Ç—å "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å PV"
df['pv_efficiency'] = df['pv_total_power'] / (df['pv1_voltage'] + df['pv2_voltage'] + 1)
```

### –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–µ–π

```python
# –í ml_model_training_example.py

# –î–ª—è PV predictor
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 15, 20],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
    RandomForestRegressor(),
    param_grid,
    cv=5,
    scoring='r2'
)

grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_
```

### –ê–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π

```python
# –ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏

from sklearn.ensemble import VotingRegressor

ensemble = VotingRegressor([
    ('rf', RandomForestRegressor(n_estimators=100)),
    ('gb', GradientBoostingRegressor(n_estimators=100)),
    ('xgb', XGBRegressor(n_estimators=100))
])

ensemble.fit(X_train, y_train)
```

---

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö

**–ü—Ä–∏–º–µ—Ä —Ä–∞—Å—á—ë—Ç–∞:**
- 1 –∑–∞–ø–∏—Å—å ‚âà 2 KB (CSV)
- 288 –∑–∞–ø–∏—Å–µ–π/–¥–µ–Ω—å √ó 2 KB = 576 KB/–¥–µ–Ω—å
- 30 –¥–Ω–µ–π ‚âà **17 MB**

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- –ê—Ä—Ö–∏–≤–∏—Ä—É–π—Ç–µ —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ (>3 –º–µ—Å—è—Ü–µ–≤)
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ JSONL –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤
- –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ parquet —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è

### –†–æ—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö

```python
# –î–æ–±–∞–≤—å—Ç–µ –≤ ml_data_collector.py

def rotate_data(self, max_days: int = 90):
    """–ê—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ä—à–µ max_days"""
    if not self.csv_path.exists():
        return
    
    df = pd.read_csv(self.csv_path)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    cutoff = datetime.now() - timedelta(days=max_days)
    old_data = df[df['timestamp'] < cutoff]
    new_data = df[df['timestamp'] >= cutoff]
    
    if len(old_data) > 0:
        archive_path = self.csv_path.parent / "archive" / f"data_{cutoff:%Y%m}.csv"
        archive_path.parent.mkdir(exist_ok=True)
        old_data.to_csv(archive_path, index=False)
        
        new_data.to_csv(self.csv_path, index=False)
        self.logger.info(f"Archived {len(old_data)} old records")
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä–∞

```python
# –£–º–µ–Ω—å—à–∏—Ç–µ —á–∞—Å—Ç–æ—Ç—É —Å–±–æ—Ä–∞ –Ω–æ—á—å—é
import datetime

def smart_interval(self) -> int:
    hour = datetime.datetime.now().hour
    
    # –ù–æ—á—å—é (22-06) - —Ä–µ–∂–µ
    if hour >= 22 or hour < 6:
        return self.collect_interval * 3  # –∫–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç
    
    # –î–µ–Ω—å - –∫–∞–∫ –æ–±—ã—á–Ω–æ
    return self.collect_interval  # –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
```

---

## Roadmap —Ä–∞–∑–≤–∏—Ç–∏—è ML-–º–æ–¥—É–ª—è

### –§–∞–∑–∞ 1: MVP ‚úÖ (—Ç–µ–∫—É—â–∞—è)
- [x] –ë–∞–∑–æ–≤—ã–π —Å–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö
- [x] –ê–Ω–∞–ª–∏–∑ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
- [x] –ü—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏ (RF, GB)
- [x] –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ run.py

### –§–∞–∑–∞ 2: Production (1-2 –º–µ—Å—è—Ü–∞)
- [ ] –°–±–æ—Ä 1+ –º–µ—Å—è—Ü –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- [ ] –û–±—É—á–µ–Ω–∏–µ production-–º–æ–¥–µ–ª–µ–π
- [ ] A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ML vs —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
- [ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏

### –§–∞–∑–∞ 3: Advanced (3-6 –º–µ—Å—è—Ü–µ–≤)
- [ ] LSTM –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- [ ] Reinforcement Learning –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- [ ] –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π retraining –º–æ–¥–µ–ª–µ–π

### –§–∞–∑–∞ 4: Ecosystem (6+ –º–µ—Å—è—Ü–µ–≤)
- [ ] API –¥–ª—è –≤–Ω–µ—à–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
- [ ] Cloud inference (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
- [ ] Multi-site learning (–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —É—Å—Ç–∞–Ω–æ–≤–æ–∫)
- [ ] Explainable AI (–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ—à–µ–Ω–∏–π –º–æ–¥–µ–ª–∏)

---

## Best Practices

### 1. –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

```python
# –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –º–æ–¥–µ–ª–∏ —Å –≤–µ—Ä—Å–∏–µ–π –∏ –¥–∞—Ç–æ–π
model_name = f"pv_predictor_v1.2_{datetime.now():%Y%m%d}.pkl"

# –•—Ä–∞–Ω–∏—Ç–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
metadata = {
    'version': '1.2',
    'trained_on': datetime.now().isoformat(),
    'train_samples': len(X_train),
    'test_r2': test_r2,
    'features': features,
    'hyperparameters': model.get_params()
}

with open('ml_data/models/metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)
```

### 2. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

```python
# –ó–∞–ø–∏—Å—ã–≤–∞–π—Ç–µ, —á—Ç–æ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∞ vs —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ
prediction_log = {
    'timestamp': datetime.now().isoformat(),
    'predicted_pv': predicted_value,
    'actual_pv': actual_value,
    'error': abs(predicted_value - actual_value),
    'model_version': '1.2'
}

# –î–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥—Ä–µ–π—Ñ–∞ –º–æ–¥–µ–ª–∏
```

### 3. Feature Store

```python
# –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
class FeatureStore:
    def __init__(self):
        self.features = {}
    
    def register_feature(self, name, func):
        """–†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é-–≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å –ø—Ä–∏–∑–Ω–∞–∫–∞"""
        self.features[name] = func
    
    def compute_all(self, raw_data):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        result = raw_data.copy()
        for name, func in self.features.items():
            result[name] = func(raw_data)
        return result

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
fs = FeatureStore()
fs.register_feature('pv_to_load_ratio', 
                   lambda df: df['pv_power'] / (df['output_power'] + 1))
```

### 4. Continuous Training

```bash
# Cron job –¥–ª—è –µ–∂–µ–º–µ—Å—è—á–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
0 2 1 * * cd /app && python app/ml/ml_model_training_example.py all >> logs/retraining.log 2>&1
```

---

## –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

### –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [ML for Time Series](https://www.tensorflow.org/tutorials/structured_data/time_series)

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
- **MLflow** - tracking —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
- **DVC** - –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- **Weights & Biases** - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è
- **SHAP** - –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

---

## –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π —á–µ–∫–ª–∏—Å—Ç

–ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –≤ production —É–±–µ–¥–∏—Ç–µ—Å—å:

- [ ] –°–æ–±—Ä–∞–Ω–æ –º–∏–Ω–∏–º—É–º **1 –º–µ—Å—è—Ü** –¥–∞–Ω–Ω—ã—Ö
- [ ] –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö < 5%
- [ ] –ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç R¬≤ > 0.7
- [ ] –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –ø—Ä–æ–π–¥–µ–Ω–æ
- [ ] –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
- [ ] –†–µ–∂–∏–º HYBRID –Ω–∞—Å—Ç—Ä–æ–µ–Ω (fallback –Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫—É)
- [ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ Grafana/Loki –Ω–∞—Å—Ç—Ä–æ–µ–Ω
- [ ] –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞
- [ ] –ö–æ–º–∞–Ω–¥–∞ –∑–Ω–∞–µ—Ç, –∫–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

---

## –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–ü—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤:

1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏: `logs/ml_controller.log`, `logs/important.log`
2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É: `python app/ml/ml_data_analyzer.py stats`
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –º–æ–¥–µ–ª–∏: `ls -lh ml_data/models/`
4. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –ø—Ä–∏–º–µ—Ä—ã –≤ –∫–æ–¥–µ (docstrings –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏)

**–£–¥–∞—á–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π! üöÄü§ñ**